---
title: 'OHI 2019 - Tourism and Recreation '
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 1
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../workflow/templates/ohi_hdr.html' 
  pdf_document:
    toc: true
editor_options: 
  chunk_output_type: console
---


[REFERENCE RMD FILE](https://cdn.rawgit.com/OHI-Science/ohiprep/master/globalprep/tr/v2018/tr_data_prep.html)


# Summary
This document describes the steps for obtaining the data used to calculate the tourism and recreation goal for the 2019 global assessment.

The general calculation is:
tr = Ep * Sr * Tw
and
Xtr = tr/90th quantile across regions

* Ep = Proportion of workforce directly employed in tourism
* Sr = (S-1)/5; Sustainability of tourism
* Tw = A penalty applied to regions with travel warnings from the US State Department (or Canada's Government Travel Advise and Advisory)


## The following data are used:

* Tourism sustainability: Travel and Tourism Competitiveness Index (TTCI) from World Economic Forum (WEF) (NOT updated for 2019)
* Proportion of workforce directly employed in tourism: World Travel & Tourism Council ([WTTC](https://www.wttc.org/datagateway))
* Travel warnings: ([U.S. State Department](https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html/) and [Canadian Government](https://travel.gc.ca/travelling/advisories))
* Per capita GDP: (World Bank with gaps filled using CIA data), used to gapfill missing values in Tourism sustainability (in previous years)


# Updates from previous assessment

## Tourism employment
WTTC data includes projections 10 years in the future, and in 2018 it was unclear when these projections began, so they used 2017 as their maximum data year. When downloading data for the 2019 assessment it was clear from the WTTC data gateway where the real data ended and projections began, so 2019 was used as the maximum data year.

We also updated the code to account for uninhabited/low population areas, as we did with the Artisanal Fishing Opportunities data prep script.

## Travel warnings
Previously, US state department data only were used to identify travel warnings for each country. In 2019 we incorporated advisories from the Canadian government to fill in data for countries the US did not establish warnings for (such as the US itself). The Canadian advisories were matched to the numeric scale used by the US State Department since 2018 (1-4, ranges from level 1 (normal precautions) to level 4 (do not travel)). 

Data on the US State Dept website span 2018 and 2019, but all of these will be considered advisory data for 2019, regardless of when they were created.

In 2019 we also gapfilled travel warnings for missing territorial regions, using administrative country data. In 2018 a multiplier of 1 was applied to each region with no data. In 2019 all of these regions also received a multiplier of 1 based on the admin country advisories.


**We were able to update the following data:**
* Tourism sustainability data from the WEC Travel and Tourism Competitiveness Report were not updated, as the 2019 report has not been released as of 15 July 2019. 

* Proportion of jobs in tourism - WTTC data reported until 2029, but 2019 is most recent year of real data (year_max) (downloaded from WTTC on 07/08/2019)

* Travel warnings for 2019 (downloaded from U.S State Department and Canadian Government on 07/02/2019)



## Initial set-up code

```{r setup, message=FALSE, warning=FALSE, results="hide"}

#library(devtools)
#devtools::install_github("ohi-science/ohicore@dev")
library(ohicore)
library(tidyverse)
library(stringr)
library(WDI)
library(here)
library(janitor)
library(plotly)
library(here)

#source common.R
source('http://ohi-science.org/ohiprep_v2019/workflow/R/common.R')

## maximum year of wttc data:
year_max <- 2019

source(here("globalprep/tr/v2019/R/tr_fxns.R"))


```


# Ts: Tourism sustainability

These data are from the World Economic Forum's "Travel and Tourism Competitiveness Report" (http://reports.weforum.org/travel-and-tourism-competitiveness-report-2017/downloads/) See mazu: _raw_data/WEF-Economics/ for more details and the raw data.

These data are not compatible across years, so only the most recent year of data is across scenarios.

These data are gapfilled using gdppcppp and UN georegion information (see next section for obtaining and preparing these data).

v2019: Skipping over this part as the WEC hasn't updated the data for 2019 yet. 
### put this in a different R markdown for just sustainability data (since it won't be updated every year)
```{r WEF processing, eval=FALSE}

# read in files
ttci_raw <- read.csv(file.path(dir_M, "git-annex/globalprep/_raw_data/WEF-Economics/d2018/WEF_TTCR17_data_for_download.csv")) %>%
  dplyr::filter(row_number() %in% c(2,598)) %>% #Identified rows and colums of interest
  dplyr::select(11:156) %>% 
  t() %>%
  data.frame() %>% 
  remove_rownames() %>%
  rename(country = "X1", score= "X2") %>% 
  mutate(score = as.numeric(as.character(score))) %>% 
  mutate(country = as.character(country))
  
#Checking the imported data is the same than in 2017! 
ttci_v2017 <-read.csv(file.path(dir_M, "git-annex/globalprep/_raw_data/WEF-Economics/d2017/wef_ttci.csv")) %>% 
  rename(score_old= score) %>%
  arrange(country) %>% 
  left_join(ttci_raw)

plot(ttci_v2017$score, ttci_v2017$score_old)
abline(0,1, col="red") #Yes! This means data has not been updated. The only difference is that now we are importing the raw data from an excel file instead of a pdf


#Changing names that are not recognized by ohicore
ttci <- ttci_raw %>%
    mutate(country = ifelse(country == "Congo, Democratic Rep.", "Democratic Republic of the Congo", country)) %>%
    mutate(country = ifelse(str_detect(country, "Ivoire"), "Ivory Coast", country))
  
  
ttci_rgn <- name_2_rgn(df_in = ttci, 
                       fld_name='country')

##Duplicated regions weighted mean
weight_data <- data.frame(country = c("China", "Hong Kong SAR"),
                          population = c(1379000000, 7347000))

ttci_rgn <- ttci_rgn %>%
  arrange(country) %>%
  left_join(weight_data, by = "country") %>%
  mutate(population = ifelse(is.na(population), 1, population)) %>%
  group_by(rgn_id, rgn_name) %>%
  summarize(score = weighted.mean(score, population)) %>%
  select(rgn_id, rgn_name, score)

head(ttci_rgn, 10)

### Save TTCI data file
write_csv(ttci_rgn, 'intermediate/wef_ttci.csv')

```

## Preparing the gdppcppp data:
These data are used to gapfill missing values in tourism sustainability.  Most of the data are from the World Bank, but CIA data fill some gaps (CIA data is available for only the most recent year).

The Artisanal Opportunities goal uses gdppcppp data, so we will get the data that was processed for that goal. (NOTE: Update the Artisanal Opportunities goal prior to preparing these data)


```{r worldbank, eval=FALSE}
wb <- read.csv("../../ao/v2018/intermediate/gdppcppp_ohi.csv") %>%
  dplyr::select(rgn_id, year, value)

```

CIA data are used to fill in missing gaps in the gdppcppp data (https://www.cia.gov/library/publications/the-world-factbook/rankorder/2004rank.html)

Downloaded: 7/16/2018

See README on the raw forlder for instructions on how to download this data. 
Note: for next year assesment, evaluate the option of scraping directly from the source website.

The following code is used to prepare these data for OHI:

```{r cia gdp, eval=FALSE}

cia_gdp <- read.csv('raw/cia_gdp_pc_ppp.csv', stringsAsFactors = FALSE, header = FALSE) %>% 
  separate(V1, c("number", "other"), sep = " ", extra = "merge") %>% 
  separate(other, c("country", "gdppcppp"), sep = "\\$") %>%
  mutate(country = str_trim(country, side = "both")) %>% 
  mutate(country = noquote(country)) %>% 
  mutate(country = as.character(country)) %>% 
  mutate(gdppcppp = as.numeric(gsub(",", "", gdppcppp))) %>% 
  select(-number) %>% 
  data.frame()

##Data reported in a lower resolution than OHI regions
splits <- data.frame(country = "Saint Helena, Ascension, and Tristan da Cunha", country2 = c("Saint Helena",                                                                "Ascension","Tristan da Cunha")) %>%
  mutate(country = as.character(country),
         country2 = as.character(country2))

cia_gdp <- cia_gdp %>%
  left_join(splits, by='country') %>%
  mutate(country2 = ifelse(is.na(country2), country, country2)) %>%
  select(country=country2, pcgdp_cia = gdppcppp)


cia_gdp_rgn <- name_2_rgn(df_in = cia_gdp, 
                       fld_name='country')

### Duplicated regions: Collapse regions after weighting by population (regions we include as a single region) - 

population_weights <- data.frame(country = c("Virgin Islands", "Puerto Rico",
                                             "China", "Hong Kong", "Macau",
                                             "Guam", "Northern Mariana Islands"),
                                 population = c(106405, 3725789,
                                         1339724852, 7071576, 636200,
                                         162896, 55023))

cia_gdp_rgn <- cia_gdp_rgn %>%
  left_join(population_weights, by="country") %>%
  mutate(population = ifelse(is.na(population), 1, population)) %>%
  group_by(rgn_id) %>%
  summarize(pcgdp_cia = weighted.mean(pcgdp_cia, population)) %>%
  ungroup() %>%
  filter(rgn_id <= 250) %>%
  select(rgn_id, pcgdp_cia)

write.csv(cia_gdp_rgn, "intermediate/wb_rgn_cia_GDPPCPPP.csv", row.names=FALSE)

```

The following code combines the two gdp datasets and gapfills missing regions using UN georegions.

If there is no World Bank gdppcppp data (pcgdp), the CIA data is used (pcgdp_cia).  The pcgdp2 variable includes both the World Bank and CIA data (with CIA data only used if there is not World Bank data).  The remaining data are estimated using UN geopolitical regions.  Ideally, the mean gdppcppp value is calculated at the r2 scale (gdp_pred_r2) using regions within each class with gdppcppp data.  If there were not enough regions with data at the r2 scale, the average at the r1 scale was used (gdp_pred_r1). The gdp_all variable combines all estimates using the following heirarchy:  World Bank -> CIA -> estimated using mean from r2 UN geopolitical regions -> estimated using mean from r1 UN geopolitical regions.    

```{r gapfill gdp, eval=FALSE}

### world bank gdp data
gdppcppp <- wb %>%
  select(rgn_id, year, pcgdp = value)

### cia gdp data
gdppcppp2 <- read.csv('intermediate/wb_rgn_cia_GDPPCPPP.csv')


### Use WB data, but if missing, use pcgdp_cia.
### combine with UN georegion data
years <- data.frame(year = min(gdppcppp$year):max(gdppcppp$year))

georegions <- ohicore::georegions

regions <- georegions %>%
  left_join(georegion_labels, by = 'rgn_id')

gdp_raw <- merge(years, regions, by=NULL) %>%
   left_join(gdppcppp, by = c('rgn_id', 'year')) %>%
  left_join(gdppcppp2, by = c("rgn_id")) 

## quick compare to make sure the CIA and World Bank data are compatible
plot(gdp_raw$pcgdp[gdp_raw$year==2017], gdp_raw$pcgdp_cia[gdp_raw$year==2017])
abline(0,1, col="red")
# a bit of scatter for a few regions, but overall looks good

gdp_raw <- gdp_raw %>%
  mutate(pcgdp2 = ifelse(is.na(pcgdp), pcgdp_cia, pcgdp))

##Calculating the means across different geopolitical levels (e.g. r2, r1)
gdp_raw <- gdp_raw %>%
  group_by(r2, year) %>%
  mutate(gdp_pred_r2 = mean(pcgdp2, na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(r1, year) %>%
  mutate(gdp_pred_r1 = mean(pcgdp2, na.rm=TRUE)) %>%
  ungroup() 

gdp_raw_gf <- gdp_raw %>%
  mutate(gdp_all = ifelse(is.na(pcgdp2), gdp_pred_r2, pcgdp2)) %>%
  mutate(gdp_all = ifelse(is.na(gdp_all), gdp_pred_r1, gdp_all)) %>%
  mutate(gapfilled = ifelse(is.na(pcgdp2) & !is.na(gdp_all), "gapfilled", NA)) %>%
  mutate(method = ifelse(is.na(pcgdp2) & !is.na(gdp_pred_r2), "UN georegion (r2)", NA)) %>%
  mutate(method = ifelse(is.na(pcgdp2) & is.na(gdp_pred_r2) & !is.na(gdp_pred_r1), "UN georegion (r1)", method)) 

write_csv(gdp_raw_gf, "intermediate/gdp_raw_gf.csv")

gdp_data_gf <- gdp_raw_gf %>%
  select(rgn_id, year, gapfilled, method) 

write_csv(gdp_data_gf, "intermediate/gdp_gf.csv")

gdp_data <- gdp_raw_gf %>%
  select(rgn_id, year, pcgdp = gdp_all)

write_csv(gdp_data, "intermediate/gdp.csv")

```


The final step is gapfilling the Sustainability data using a linear model with gdppcppp and UN geopolitical regions as predictor variables.  

```{r, eval=FALSE}

sust  <- read.csv('intermediate/wef_ttci.csv', stringsAsFactors = FALSE)

### don't need to gapfill data without tourism data:
## Most recent tourism data is 2018.  

ep_gf <- read.csv("output/tr_jobs_pct_tourism.csv") %>%
  filter(year == 2017) %>%
  select(rgn_id, Ep) %>%
  filter(!is.na(Ep))

# gdp dataframe prepared above (World Bank, CIA, and gapfilled gdp data)
gdp_raw_gf <- read.csv("intermediate/gdp_raw_gf.csv", stringsAsFactors = FALSE) %>% 
  filter(year == 2017) %>%
  select(rgn_id, r0_label, r1_label, r2_label, rgn_label, pcgdp, pcgdp_cia, pcgdp2, gdp_all) 

tr_sust <- gdp_raw_gf %>%
           left_join(sust, by = c("rgn_id")) %>%
          left_join(ep_gf, by = c("rgn_id")) %>%  
          rename(S_score = score) %>%
          filter(rgn_id != 213)

### Add gapfill flag variable 
##Reminder:
## pcgdp2: includes both the World Bank and CIA data (with CIA data only used if there is not World Bank data)
## Ep: Proportion of workforce directly employed in tourism
##S_score: tourism sustainability score

tr_sust_gf <- tr_sust %>%
  mutate(gapfilled = ifelse(is.na(S_score) & !is.na(Ep), "gapfilled", NA)) %>%
  mutate(method = ifelse(is.na(S_score) & !is.na(Ep) & is.na(pcgdp2), "lm georegion + gdppcppp, with est. gdppcppp", NA)) %>%
  mutate(method = ifelse(is.na(S_score) & !is.na(Ep) & !is.na(pcgdp2), "lm georegion + gdppcppp", method)) %>%
  select(rgn_id, gapfilled, method)

write.csv(tr_sust_gf, "output/tr_sustainability_gf.csv", row.names=FALSE)

```
  

### Gapfilling
Linear models using gdppcppp and UN geopolitical regions as predictor variables. However if there is no gdppc data we estimate the gdppc using the UN georegions and then used in the linear model to gapfill the sustainability score.

```{r, eval=FALSE}

### Gapfill S using r1 and/or r2 regional data and PPP-adjusted per-capita GDP
### Looked at models with a year variable, but wasn't significant and decided to exclude

mod3 <- lm(S_score ~ as.factor(r2_label) + gdp_all, data=tr_sust, na.action = na.exclude)
summary(mod3)
anova(mod3)

mod4 <- lm(S_score ~ as.factor(r1_label) + gdp_all, data=tr_sust, na.action = na.exclude)
summary(mod4)
anova(mod4)

plot(predict(mod3), tr_sust$S_score)
abline(0,1)
plot(predict(mod4), tr_sust$S_score)
abline(0,1)


## Estimate missing data and gapfill
# Some of the r1 levels do not have data and consequently causes a fail. This chink of code  drop these levels so an NA is returned

# Select only r2 column
new_data <- tr_sust %>% 
  dplyr::select(r2_label, gdp_all)

unique(tr_sust$r2_label)

r2_w_data <- unique(tr_sust$r2_label[!is.na(tr_sust$S_score)])
  
new_data_r2 <- new_data %>%
  mutate(r2_label = ifelse(r2_label %in% r2_w_data, r2_label, NA))

# Predict sustainability scores using linear model 3 (using r2 data)
tr_sust <- tr_sust %>% 
  dplyr::mutate(S_score_pred_r2 = predict(mod3, newdata = new_data_r2))


# Select only r1 column
new_data <- tr_sust %>% 
  dplyr::select(r1_label, gdp_all)

unique(tr_sust$r1_label)

r1_w_data <- unique(tr_sust$r1_label[!is.na(tr_sust$S_score)])

new_data_r1 <- new_data %>%
  mutate(r1_label = ifelse(r1_label %in% r1_w_data, r1_label, NA))

# Predict sustainability scores using linear model 4 (using r1 data)
tr_sust <- tr_sust %>% 
  dplyr::mutate(S_score_pred_r1 = predict(mod4, newdata = new_data_r1))



## some are missing the r1 predictions, but none of these have Ep scores, so not relevant
filter(tr_sust, is.na(S_score_pred_r1))

tr_sust <- tr_sust %>%
  mutate(S_score_2 = ifelse(is.na(S_score), S_score_pred_r2, S_score)) %>%
  mutate(S_score_2 = ifelse(is.na(S_score_2), S_score_pred_r1, S_score_2)) %>%
  mutate(year = '2018') %>%
#  filter(!is.na(Ep)) %>%
  select(rgn_id, year, S_score=S_score_2)

summary(tr_sust)

write_csv(tr_sust, "output/tr_sustainability.csv")
```


## Compare with previous year of data 

```{r, eval=FALSE}

compare <- read.csv("../v2017/output/tr_sustainability.csv") %>% # change year
  select(rgn_id, old_S_score = S_score) %>%
  left_join(tr_sust, by = "rgn_id") %>%
  mutate(dif = old_S_score - S_score)

plot(compare$S_score, compare$old_S_score)
abline(0, 1, col="red")
#looks reasonable 

```


# Ep: Proportion of workforce directly employed in tourism

These data are from the [World Travel & Tourism Council](http://www.wttc.org/datagateway).  We use "direct" employment data (see mazu: git-annex/globalprep/_raw_data/WTTC/d2019/README.md for instructions on obtaining data). The data extend to 2029, which includes 10 years of projections. The actual data goes to 2019 (projected/real data are differentiated on the data gateway chart).

These data are cleaned and formatted using the R/process_WTTC.R script. Missing values are gapfilled using the UN georegion information.

```{r wttc prop tourism, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}

## describe where the raw data are located:
scenario_yr <- "v2019"
dir_wttc <- file.path(dir_M, 'git-annex/globalprep/_raw_data/WTTC/d2019/raw')
dir_github <- here("globalprep/tr", scenario_yr)

## processing script that formats the WTTC for OHI, saves the following: intermediate/wttc_empd_rgn.csv
source(here("globalprep/tr/v2019/R/process_WTTC.R"))
##### v2019: Duplicates = Guadeloupe/Martinique, PR/VI, China (accounted for in the script)

## read in the dataset created by above function:
tr_jobs_pct_tour <- read_csv(here('globalprep/tr/v2019/intermediate/wttc_empd_rgn.csv')) %>% 
 dplyr::select(rgn_id, year, jobs_pct)

## format data to have complete years/regions and convert percentage of jobs to proportion of jobs
rgn_names <- read_csv('https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/eez/spatial/regions_list.csv') %>%
    dplyr::select(rgn_id)

# create data frame with a row for each combination of region id and year
rgn_names <- expand.grid(rgn_id = rgn_names$rgn_id, 
                             year= min(tr_jobs_pct_tour$year):max(tr_jobs_pct_tour$year)) 
      
tr_data_raw <- rgn_names %>%
  full_join(tr_jobs_pct_tour %>%
                rename(Ep = jobs_pct) %>%
                mutate(Ep = Ep/100) %>%
                mutate(Ep = ifelse(Ep > 1, NA, Ep)),
              by = c('rgn_id', 'year')) %>%
  filter(!rgn_id == 213) %>% 
  filter(!rgn_id == 255) # ditch disputed regions and Antarctica

## v2019: >2500 NAs in Ep column
# summary(tr_data_raw)


## gapfill missing data using UN georegion data:
georegions       <- georegions
georegion_labels <- georegion_labels

tr_data_raw <- tr_data_raw %>%
  left_join(georegions, by = 'rgn_id') %>%
  left_join(georegion_labels, by = 'rgn_id') %>%
  select(-r0)

# Calculate two different gapfill columns using r2 and r1
tr_data_raw_gf <- tr_data_raw %>%
  group_by(year, r2) %>%
  mutate(Ep_pred_r2 = mean(Ep, na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(year, r1) %>%
  mutate(Ep_pred_r1 = mean(Ep, na.rm=TRUE)) %>%
  ungroup()

# first gapfill with r2, if no value available use r1; create column indicating whether value was gapfilled and if so, by what method.
tr_data_raw_gf <- tr_data_raw_gf %>%
  mutate(Ep_all = ifelse(is.na(Ep), Ep_pred_r2, Ep)) %>%
  mutate(Ep_all = ifelse(is.na(Ep_all), Ep_pred_r1, Ep_all)) %>% 
  mutate(gapfilled = ifelse(is.na(Ep) & !is.na(Ep_all), "gapfilled", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & !is.na(Ep_pred_r2), "UN georegion (r2)", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & is.na(Ep_pred_r2) & !is.na(Ep_pred_r1), "UN georegion (r1)", method)) 

######################################

### After gap-filling, make sure low/uninhabited regions are NA
# Create df for unpopulated/low populated regions
low_pop()
low_pop <- low_pop %>%
  filter(est_population < 3000 | is.na(est_population)) %>%  #filter out regions that have populations > 3000 and keep NA values 
  rename(rgn_label = rgn_nam)


# make sure all the NAs are uninhabited regions
tr_data_nas <- tr_data_raw_gf %>% 
  filter(is.na(Ep_all)) %>% 
  select(rgn_id, year, r1_label, r2_label, rgn_label) %>% 
  left_join(low_pop, by = c("rgn_id", "rgn_label"))
#  filter(!duplicated(rgn_id))

max(tr_data_nas$est_population, na.rm=TRUE)<3000 # should be true

# make sure all the uninhabited regions are NA (along with gapfill and method if they were gapfilled above)
tr_data_raw_gf <- tr_data_raw_gf %>% 
  mutate(Ep_all = ifelse(rgn_id %in% low_pop$rgn_id, NA, Ep_all)) %>% 
  mutate(gapfilled = ifelse(is.na(Ep_all), NA, gapfilled)) %>% 
  mutate(method = ifelse(is.na(Ep_all), NA, method))


# check NAs once more 
# summary(tr_data_raw_gf)
# v2019: Adding the low pop df identifies 13 additional regions that should be NA instead of gapfilled, taking the total number of NAs in the data set from 245 to 700


# save the gapfill report data
tr_data_gf <- tr_data_raw_gf %>%
  select(rgn_id, year, gapfilled, method) 

write_csv(tr_data_gf, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism_gf.csv"))

tr_data <- tr_data_raw_gf %>%
  select(rgn_id, year, Ep=Ep_all) 



# Save gap-filled data
write_csv(tr_data, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv"))

```

## Data check and outlier investigation
```{r data check, message=FALSE, echo=FALSE}


## A quick check to make sure last year's values aren't too crazy different
## (NOTE: the source data has been updated, so there are some changes, but they shouldn't be super different)

old <- read_csv(here('globalprep/tr/v2018/output/tr_jobs_pct_tourism.csv')) %>%
  select(rgn_id, year, ep_old=Ep)

new <- read_csv(here('globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv')) %>%
  left_join(old) %>%
  filter(year==2018) %>%
  arrange(ep_old)

# Visualize data comparison 
ggplotly(ggplot(new, aes(x = Ep, y = ep_old, labels = rgn_id)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red"))


######## investigate outliers (if applicable)

new_diff <- new %>%
  mutate(difference = ep_old-Ep) %>% 
  filter(!is.na(ep_old)) %>% 
  filter(!is.na(Ep))

outliers <- tr_data_raw %>% 
  filter(rgn_id %in% c(15,116,117,118,119,122,125,127,173,250)) %>% 
  filter(year == 2018) %>% 
  select(rgn_id, rgn_label) %>% 
  left_join(new, by = "rgn_id", "year") %>% 
  mutate(difference = Ep-ep_old) %>% 
  mutate(avg_diff = mean(new_diff$difference))

### v2019: These outliers can be explained by changes in the source data. Compared raw data from v2018 and all of these regions have large changes in the % share of total employment reported for the year 2018. WTTC likely backfills and adjusts their data on a yearly basis.  
```



# Tw: Travel warnings

Primary source of information is from the [U.S. State Department](https://travel.state.gov/content/passports/en/alertswarnings.html), secondary source is the [Canadian Government](https://travel.gc.ca/travelling/advisories)


**For future assessments** It would be worthwhile to see if data can be "scraped" directly from the government websites into R. This seems possible given the new format of the state department travel warning data.

### Getting data for 2019 assessment
The following code is used transform the warnings into a multiplier that is used to calculate tourism and recreation scores. Data from each country are copied from the US and Canada government travel websites, pasted into an excel file, and saved as a .csv in the raw folder (tr_travelwarning_20??_raw.csv)

*Date downloaded*: 1 July 2019

*Date range of warnings*: 18 June 2018 - 1 July 2019 (note: regardless of date of the warning, the advisory year will be the assessment year)


## After raw data are uploaded, wrangle and clean the new data:

```{r, eval=FALSE}
##Reading and wrangling 2019 warning data

warn_raw <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2019_raw.csv'), na.strings = " ") %>% 
  mutate(country = as.character(country)) 

# Remove text information from level and filter out regional warnings
warn_clean <- warn_raw %>% 
  mutate(level = as.numeric(str_extract(level, '[1,2,3,4]'))) %>% 
  filter(!(regional %in% 1)) %>% # remove regions that have regional warnings, as those are no longer considered in the assessment
  select(assess_year, level, country) %>% 
  rename(year = assess_year)


## Correct regions that are reported together - check to make sure these are necessary and that everything is covered as data change from year to year. Also make sure level data is coming through for each. 


french_indies <- data.frame(country="French West Indies", 
                            country_new =c("Northern Saint-Martin")) %>%
  left_join(filter(warn_clean, country=="French West Indies")) %>%
  select(country=country_new, year, level)

BES <- data.frame(country="Bonaire, Sint Eustatius and Saba", 
                            country_new =c("Saba", "Sint Eustatius")) %>% # Bonaire already reported separately 
  left_join(filter(warn_clean, country=="Bonaire, Sint Eustatius and Saba")) %>%
    select(country=country_new, year, level)


line <- data.frame(country="Line Islands (Kiribati)", 
                            country_new =c("Line Group", "Phoenix Group")) %>%
  left_join(filter(warn_clean, country=="Line Islands (Kiribati)")) %>%
    select(country=country_new, year, level)
# These are not the region names reported in OHI, but they are used in the name_2_rgn function
  
warn_improved <- filter(warn_clean, country != "French West Indies") %>%
  bind_rows(french_indies) 

warn_improved <- filter(warn_improved, country != "Bonaire, Sint Eustatius and Saba") %>%
  bind_rows(BES)

warn_improved <- filter(warn_improved, country != "Line Islands (Kiribati)") %>%
  bind_rows(line)



##Correct names for regions not identified by the name_2_region, based off of error messages after running function below
# Change names to match those reported by OHI (not always necessary)
warn_improved <- warn_improved %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Israel"), "Israel", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country, "^Republic of the Congo"), "Republique du Congo", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"U.S. Virgin Islands"), "Puerto Rico and Virgin Islands of the United States", country)) %>% # creates duplicate of PR/VI with same warning level
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Puerto Rico"), "Puerto Rico and Virgin Islands of the United States", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Guadeloupe"), "Guadeloupe and Martinique", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Saint Vincent and The Grenadines"), "Saint Vincent and the Grenadines", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Burma"), "Myanmar", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"North Korea"), "North Korea", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Solomon Island"), "Solomon Islands", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Guam"), "Northern Mariana Islands and Guam", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Northern Mariana Islands"), "Northern Mariana Islands and Guam", country))
  

# Look at warn_improved and remove other regions that are duplicated or aren't reported in the OHI. Make sure there are no NAs. 
get_dupes(warn_improved)

warn_improved <- warn_improved %>% 
  filter(!duplicated(country))

write_csv(warn_improved, here("globalprep/tr/v2019/intermediate/warning_2019.csv"))


```


## Transform the warnings into a multiplier that is used to calculate tourism and recreation scores.

Travel warning  | Multiplier   | Description
--------------- | ------------ | -------------------
Level 1 | 1 (no penalty) | Exercise Normal Precautions: This is the lowest advisory level for safety and security risk. There is some risk in any international travel. 
Level 2 | 1 (no penalty) | Exercise Increased Caution:  Be aware of heightened risks to safety and security. 
Level 3 | 0.25  | Reconsider Travel: Avoid travel due to serious risks to safety and security.
Level 4 | 0 (full penalty, results in zero scores) | Do Not Travel:  This is the highest advisory level due to greater likelihood of life-threatening risks. 

```{r, eval=FALSE}
warn_complete <- read_csv(here("globalprep/tr/v2019/intermediate/warning_2019.csv"))

scores <-  data.frame(level = c(1, 2, 3, 4), multiplier = c(1, 1, 0.25, 0)) 


warn_multiplier <-  warn_complete %>%  
  left_join(scores, by="level") %>% 
  group_by(year, country) %>%
  mutate(warning_count = n()) %>%
  ungroup()

# Check to see if there are regions with more than one warning (in general there should be no regions with more than one advisory, but some are combined after being reported separately and may have different advisories) 

warn_count <- filter(warn_multiplier, warning_count>1)

# If warn_count >0, multipliers from duplicate regions will be averaged: 
warn_multiplier <- warn_multiplier %>%
  group_by(year, country) %>%
  summarize(multiplier = mean(multiplier))

#Save file with 2019 multiplier data
write_csv(warn_multiplier, here("globalprep/tr/v2019/intermediate/warning.csv"))

```


## Convert names to OHI regions and clean. 

```{r travel warnings, message=FALSE, echo=FALSE, results="hide"}
warn <- read_csv(here("globalprep/tr/v2019/intermediate/warning.csv"))

#Add rgn_id
warn_rgn <- name_2_rgn(df_in = warn, 
                       fld_name='country', 
                       flds_unique=c('country','year'))
# Double check error message to make sure all of the landlocked regions are indeed landlocked, and that everything not found in the lookup table is not a region reported by OHI. 
                                                  
# Check to see if any regions are duplicated:
sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))
# China has multiple warnings (rgn_id 209)

# Average warnings for China, China Macao SAR and Hong Kong: 
warn_rgn <- warn_rgn %>%
  group_by(rgn_id, rgn_name, year) %>%
  summarize(multiplier = mean(multiplier)) %>%
  ungroup()

# Check again to see if there are any duplicate multipliers
sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))

```


## Final step: Compare with previous year's data

Many European regions now have a travel warning due to increased terrorism (e.g., United Kingdom, Italy, Spain, Germany), although this doesn't show up in the following figure because previously, these regions had no travel warning (and were thus, NA).

The change in not penalizing subregional warnings tended to reduce the penalty (i.e. increase the multiplier value).
```{r, message=FALSE, echo=FALSE}
# The following indicates changes over time as well as changes to the State Department's approach to quantifying risk

tmp_old <- read_csv(here("globalprep/tr/v2018/output/tr_travelwarnings.csv")) %>% 
  spread(year, multiplier) %>%
  data.frame()
  
tmp <- warn_rgn %>%
  spread(year, multiplier) %>%
  data.frame() %>% 
  left_join(tmp_old, by = "rgn_id")

# Add noise to scatter to compare warnings between this year and last year (clustered points indicate where there are multiple observations of the same value)
ggplotly(ggplot(tmp, aes(x = jitter(X2018), y = jitter(X2019), labels = rgn_id)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red"))

# Investigate outliers (will potentially change from year to year)
outliers <- tmp %>% 
  filter(rgn_id %in% c(49, 73, 112, 136, 139)) %>% 
  select(rgn_id, rgn_name, X2018, X2019)

# Makes sense; these regions have all experienced stability changes in the past year 

```

## Gapfill territorial regions with admin country data and save the travel warning data in the output folder

```{r, eval=FALSE}
georegions <- georegion_labels %>%
  select(rgn_id)
  
warn_rgn_spread <- warn_rgn %>%
  spread(year, multiplier) %>%
  full_join(georegions, by=c("rgn_id")) %>%
  data.frame() %>%
  gather(year, multiplier, starts_with("X")) %>%
  mutate(year = gsub("X", "", year)) %>%
  filter(rgn_id <= 250) %>%
  filter(rgn_id != 213) # Filter out Antarctica
  
# Check number of regions reported - should be 220 
table(warn_rgn_spread$year) 


# Identify territories without advisories and connect them with multipliers for admin regions 
region_data() # reload common.R if this isn't working
warn_rgn_nas <- warn_rgn_spread %>% 
  filter(is.na(rgn_name)) %>% 
  select(rgn_id) %>% 
  left_join(rgns_eez, by = "rgn_id") %>% 
  select(rgn_id, rgn_name, admin_rgn_id, admin_country_name)

admin_rgn_multipliers <- warn_rgn_nas %>% 
  select(rgn_id = admin_rgn_id) %>% 
  left_join(warn_rgn_spread, by = "rgn_id") %>% 
  rename(admin_rgn_id = rgn_id) %>% 
  select(admin_rgn_id, year, multiplier) %>%
  filter(!duplicated(admin_rgn_id))

warn_rgn_nas <- warn_rgn_nas %>% 
  left_join(admin_rgn_multipliers, by = "admin_rgn_id") %>% 
  select(rgn_id, rgn_name, year, multiplier)


### Finalize warnings data and save
# Remove NAs from warn_rgn_spread, then add them back in

warn_rgn_spread <- warn_rgn_spread %>%
  filter(!is.na(rgn_name)) %>% 
  bind_rows(warn_rgn_nas)
  
warn_rgn_all_rgns <- warn_rgn_spread %>%
  select(rgn_id, year, multiplier) %>%
  arrange(year, rgn_id) %>% 
  mutate(year = as.numeric(year))

# Check again that we have 220 regions reported
table(warn_rgn_all_rgns$year) 


# Save 2019 data
write_csv(warn_rgn_all_rgns, here('globalprep/tr/v2019/output/tr_travelwarnings_2019only.csv'))


## Create gapfill file
# Add information about gapfill method to regions that were filled based on administrative country advisory above

travelwarning_gf <- read_csv(here("globalprep/tr/v2019/output/tr_travelwarnings.csv")) %>% 
  mutate(gapfilled = ifelse(rgn_id %in% warn_rgn_nas$rgn_id, 1, 0)) %>% 
  mutate(method = ifelse(gapfilled == 1, "Gapfilled based on administrative country advisory", NA)) %>% 
  select(-multiplier)

write_csv(travelwarning_gf, here('globalprep/tr/v2019/output/tr_travelwarnings_gf.csv'))


## Combine with previous years' data and save

travel_warnings_all <- read_csv(here("globalprep/tr/v2018/output/tr_travelwarnings.csv")) %>%
  bind_rows(warn_rgn_all_rgns)

write_csv(travel_warnings_all, here('globalprep/tr/v2019/output/tr_travelwarnings.csv'))

```

